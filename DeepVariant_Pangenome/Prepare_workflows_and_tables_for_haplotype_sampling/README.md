## Preparing workflows for haplotype sampling and making GBZ files with different number of haplotypes
### Comment 1 : 01/29/2023

#### Modifying the haplotype sampling workflow
I created a fork of https://github.com/vgteam/vg_wdl in here https://github.com/mobinasri/vg_wdl to be able to make the changes I want in the workflows.
I made a customized version of Parsa's workflow for haplotype sampling. The original version is here: https://github.com/vgteam/vg_wdl/blob/master/workflows/haplotype_sampling.wdl and 
my version is here: https://github.com/mobinasri/vg_wdl/blob/master/workflows/haplotype_sampling_customized.wdl

The customized version can take an array of read files of any type. I imported `extract_reads.wdl` for extracting each read file, which can be either BAM,CRAM,FASTQ or FASTA.GZ
and receiving a fastq file. This haplotype sampling workflow will take a list of haplotype numbers and will create an array of gbz files; each one is a graph created with 
a different haplotype number. My main motivation for writing this WDL was that we are going to train multiple models with different number of haplotypes and then investigate which one acheives
a higher accuracy. For this aim we need to create multiple gbz files for different haplotype numbers. With this WDL we can run the workflow only once and create all the graphs 
that we need. 

#### Preparing csv files for tesing the workflow

I'm going to use one of the bam files Parsa generated by mapping short reads to GRCh38 with Giraffe. I will use this one:
```
gs://pepper-deepvariant/seeskand/dv_training/HG003_35x_merged.positionsorted.bam
gs://pepper-deepvariant/seeskand/dv_training/HG003_35x_merged.positionsorted.bam.bai
```

I made these two csv files for my run on Phoenix:
[data_table.csv](https://github.com/mobinasri/research_notes/blob/main/DeepVariant_Pangenome/Prepare_workflows_and_tables_for_haplotype_sampling/files/data_table.csv)
and
[input_mapping.csv](https://github.com/mobinasri/research_notes/blob/main/DeepVariant_Pangenome/Prepare_workflows_and_tables_for_haplotype_sampling/files/input_mapping.csv)

I ran the commands below
```
# set env variables
USER_NAME="masri"
EMAIL="masri@ucsc.edu"
WDL_PATH="/private/groups/patenlab/masri/apps/vg_wdl/workflows/haplotype_sampling_customized.wdl"
WDL_NAME=$(basename ${WDL_PATH%%.wdl})
INPUT_MAPPING_CSV="/private/groups/patenlab/masri/haplotype_sampling/HG003/input_mapping.csv"
INPUT_DATA_TABLE_CSV="/private/groups/patenlab/masri/haplotype_sampling/HG003/data_table.csv"
WORKING_DIR="/private/groups/patenlab/masri/haplotype_sampling/HG003"
```
```
# get the scripts for running my job
cd /private/groups/patenlab/masri/apps
git clone https://github.com/human-pangenomics/hprc_intermediate_assembly
LAUNCH_FROM_TABLE_PY="/private/groups/patenlab/masri/apps/hprc_intermediate_assembly/hpc/launch_from_table.py"
LAUNCH_WORKFLOW_JOB_ARRAY_BASH="/private/groups/patenlab/masri/apps/hprc_intermediate_assembly/hpc/launch_workflow_job_array_single_machine.sh"
```
```
# create directories for putting input and output json files
mkdir -p ${WORKING_DIR}
cd ${WORKING_DIR}

mkdir -p ${WDL_NAME}_output_jsons
mkdir -p ${WDL_NAME}_input_jsons

# create input json files
cd ${WORKING_DIR}/${WDL_NAME}_input_jsons
python3  ${LAUNCH_FROM_TABLE_PY} \
    --data_table ${INPUT_DATA_TABLE_CSV} \
    --field_mapping ${INPUT_MAPPING_CSV} \
    --workflow_name ${WDL_NAME}

```
```
# Run the job
cd ${WORKING_DIR}
cd ${WDL_NAME}_output_jsons
INPUT_JSON_DIR=${WORKING_DIR}/${WDL_NAME}_input_jsons
mkdir -p ${WDL_NAME}_logs

sbatch      --job-name=${WDL_NAME}_${USERNAME} \
            --cpus-per-task=16 \
            --mem=180G \
            --mail-user=${EMAIL} \
            --output=${WDL_NAME}_logs/${WDL_NAME}_%A_%a.log \
            --array=1-1%1  \
            --partition=long  \
            --time=3-00:00:00 \
            ${LAUNCH_WORKFLOW_JOB_ARRAY_BASH} \
            --wdl ${WDL_PATH} \
            --sample_csv  ${INPUT_DATA_TABLE_CSV} \
            --input_json_dir ${INPUT_JSON_DIR}
```


### Comment 2 : 07/16/2024

I updated the csv file to contain all 7 GIAB samples. [data_table.csv](https://github.com/mobinasri/research_notes/blob/main/DeepVariant_Pangenome/Prepare_workflows_and_tables_for_haplotype_sampling/files/data_table.csv)

Then I ran the commands below to submit jobs for haplotype sampling using the workflow https://github.com/mobinasri/vg_wdl/blob/master/workflows/haplotype_sampling_customized.wdl

Note that in these commands newer versions of the scripts launch_from_table.py and launch_workflow_job_array_single_machine.sh are used.

#### Commands to generate input json files:

```
cd /private/groups/patenlab/masri/haplotype_sampling/HG003
WORKING_DIR=${PWD}

## Make sure you are in the working directory. Check step 1 for setting ${WORKING_DIR} if it's not set already
cd ${WORKING_DIR}

## Get the script for creating input json files.
wget https://raw.githubusercontent.com/human-pangenomics/hprc_intermediate_assembly/1f61ff0043442d8350a282ef3533def588bee8dc/hpc/launch_from_table.py

WDL_PATH=/private/groups/patenlab/masri/apps/vg_wdl/workflows/haplotype_sampling_customized.wdl
WDL_FILENAME=$(basename ${WDL_PATH})
WDL_NAME=${WDL_FILENAME%%.wdl}

## Make a directory for saving input json files
mkdir -p ${WDL_NAME}_input_jsons
cd ${WDL_NAME}_input_jsons

## Make input json files
## One json will be created per row
python3  ${WORKING_DIR}/launch_from_table.py \
            --data_table ${WORKING_DIR}/data_table.csv \
            --field_mapping ${WORKING_DIR}/input_mapping.csv \
            --workflow_name ${WDL_NAME}


```

#### Commands to submit slurm jobs:

```
## Make sure you are in the working directory. Check step 1 for setting ${WORKING_DIR} if it's not set already
cd ${WORKING_DIR}

## Get the bash script for running WDLs on Slurm using Toil
wget https://raw.githubusercontent.com/human-pangenomics/hprc_intermediate_assembly/b81bbb9540eaf5632a53faba43be71a0974f14f6/hpc/toil_sbatch_single_machine.sh

## Set environment variables for sbatch
USERNAME="masri"
EMAIL="masri@ucsc.edu"
TIME_LIMIT="70:00:00"

## Partition should be modifed based on the available partitions on the server
PARTITION="long"

## Go to the execution directory
mkdir -p ${WDL_NAME}_logs

## Run jobs arrays
## --array=1-7%7 will make 7 jobs; one per input json file (numbered by row indices in csv file)
sbatch      --job-name=${WDL_NAME}_${USERNAME} \
            --cpus-per-task=64 \
            --mem=256G \
            --mail-user=${EMAIL} \
            --output=${WDL_NAME}_logs/${WDL_NAME}_%A_%a.log \
            --array=1-7%7  \
            --time=${TIME_LIMIT} \
            --partition=${PARTITION} \
            ${WORKING_DIR}/toil_sbatch_single_machine.sh \
            --wdl ${WDL_PATH} \
            --sample_csv  ${WORKING_DIR}/data_table.csv \
            --input_json_path ${WORKING_DIR}/${WDL_NAME}_input_jsons/\${SAMPLE_ID}_${WDL_NAME}.json

```


The gbz files generated are listed here:
```
cd /private/groups/patenlab/masri/haplotype_sampling/HG003
find . | grep "\.gbz" | grep analysis
./HG002_HPRC_v1.1.hap_sampled/analysis/haplotype_sampling_customized_outputs/HG002_HPRC_v1.1.hap_sampled.hap_num_4.gbz
./HG002_HPRC_v1.1.hap_sampled/analysis/haplotype_sampling_customized_outputs/HG002_HPRC_v1.1.hap_sampled.hap_num_16.gbz
./HG002_HPRC_v1.1.hap_sampled/analysis/haplotype_sampling_customized_outputs/HG002_HPRC_v1.1.hap_sampled.hap_num_2.gbz
./HG002_HPRC_v1.1.hap_sampled/analysis/haplotype_sampling_customized_outputs/HG002_HPRC_v1.1.hap_sampled.hap_num_8.gbz
./HG006_HPRC_v1.1.hap_sampled/analysis/haplotype_sampling_customized_outputs/HG006_HPRC_v1.1.hap_sampled.hap_num_4.gbz
./HG006_HPRC_v1.1.hap_sampled/analysis/haplotype_sampling_customized_outputs/HG006_HPRC_v1.1.hap_sampled.hap_num_8.gbz
./HG006_HPRC_v1.1.hap_sampled/analysis/haplotype_sampling_customized_outputs/HG006_HPRC_v1.1.hap_sampled.hap_num_2.gbz
./HG006_HPRC_v1.1.hap_sampled/analysis/haplotype_sampling_customized_outputs/HG006_HPRC_v1.1.hap_sampled.hap_num_16.gbz
./HG003_HPRC_v1.1.hap_sampled/analysis/haplotype_sampling_customized_outputs/HG003_HPRC_v1.1.hap_sampled.hap_num_16.gbz
./HG003_HPRC_v1.1.hap_sampled/analysis/haplotype_sampling_customized_outputs/HG003_HPRC_v1.1.hap_sampled.hap_num_4.gbz
./HG003_HPRC_v1.1.hap_sampled/analysis/haplotype_sampling_customized_outputs/HG003_HPRC_v1.1.hap_sampled.hap_num_8.gbz
./HG003_HPRC_v1.1.hap_sampled/analysis/haplotype_sampling_customized_outputs/HG003_HPRC_v1.1.hap_sampled.hap_num_2.gbz
./HG001_HPRC_v1.1.hap_sampled/analysis/haplotype_sampling_customized_outputs/HG001_HPRC_v1.1.hap_sampled.hap_num_8.gbz
./HG001_HPRC_v1.1.hap_sampled/analysis/haplotype_sampling_customized_outputs/HG001_HPRC_v1.1.hap_sampled.hap_num_4.gbz
./HG001_HPRC_v1.1.hap_sampled/analysis/haplotype_sampling_customized_outputs/HG001_HPRC_v1.1.hap_sampled.hap_num_16.gbz
./HG001_HPRC_v1.1.hap_sampled/analysis/haplotype_sampling_customized_outputs/HG001_HPRC_v1.1.hap_sampled.hap_num_2.gbz
./HG007_HPRC_v1.1.hap_sampled/analysis/haplotype_sampling_customized_outputs/HG007_HPRC_v1.1.hap_sampled.hap_num_4.gbz
./HG007_HPRC_v1.1.hap_sampled/analysis/haplotype_sampling_customized_outputs/HG007_HPRC_v1.1.hap_sampled.hap_num_2.gbz
./HG007_HPRC_v1.1.hap_sampled/analysis/haplotype_sampling_customized_outputs/HG007_HPRC_v1.1.hap_sampled.hap_num_8.gbz
./HG007_HPRC_v1.1.hap_sampled/analysis/haplotype_sampling_customized_outputs/HG007_HPRC_v1.1.hap_sampled.hap_num_16.gbz
./HG005_HPRC_v1.1.hap_sampled/analysis/haplotype_sampling_customized_outputs/HG005_HPRC_v1.1.hap_sampled.hap_num_8.gbz
./HG005_HPRC_v1.1.hap_sampled/analysis/haplotype_sampling_customized_outputs/HG005_HPRC_v1.1.hap_sampled.hap_num_16.gbz
./HG005_HPRC_v1.1.hap_sampled/analysis/haplotype_sampling_customized_outputs/HG005_HPRC_v1.1.hap_sampled.hap_num_4.gbz
./HG005_HPRC_v1.1.hap_sampled/analysis/haplotype_sampling_customized_outputs/HG005_HPRC_v1.1.hap_sampled.hap_num_2.gbz
./HG004_HPRC_v1.1.hap_sampled/analysis/haplotype_sampling_customized_outputs/HG004_HPRC_v1.1.hap_sampled.hap_num_8.gbz
./HG004_HPRC_v1.1.hap_sampled/analysis/haplotype_sampling_customized_outputs/HG004_HPRC_v1.1.hap_sampled.hap_num_16.gbz
./HG004_HPRC_v1.1.hap_sampled/analysis/haplotype_sampling_customized_outputs/HG004_HPRC_v1.1.hap_sampled.hap_num_2.gbz
./HG004_HPRC_v1.1.hap_sampled/analysis/haplotype_sampling_customized_outputs/HG004_HPRC_v1.1.hap_sampled.hap_num_4.gbz
```

There are 4 gbz files per sample (28 in total). 

These gbz files are uploaded to `gs://pepper-deepvariant/mobinasri/haplotype_sampling/giab_samples`


### Comment 3 : 07/26/2024
In all my previous jobs diploid sampling was turned on and I didn't know that with this option we can only get two haplotypes in the final gbz file and the number of haplotypes passed as an input parameter is only for specifying the number of the candidate haplotypes which are selected initially.To have more haplotypes I ran the same workflow using the same csv files however this time with diploid sampling turned off (the related line in data_table.csv is modified). The reason is that with diploid sampling turned we can only get two haplotypes in the final gbz file but for DeepVariant we like to train and test pangenome-aware models with varying number of haplotypes. 

The jobs are finished and the final gbz files (28 files) are uploaded to the same bucket but in a new subdirectory: `gs://pepper-deepvariant/mobinasri/haplotype_sampling/giab_samples/non_diploid_sampling`

### Comment 4 : 11/19/2024
In order to test pangenome-aware DeepVariant with more number of haplotypes I ran non-diploid sampling this time for `32` and `64` haplotypes. We saw that 16 had the best performance with DeepVariant so I wanted to check if 32 and 64 perform even better or not. I modified the line related to `HAPLOTYPE_NUMBER_ARRAY` in `input_mapping.csv`
```
HaplotypeSampling.HAPLOTYPE_NUMBER_ARRAY,array,"[32, 64]"
```

#### Create json files
```
cd /private/groups/patenlab/masri/haplotype_sampling/giab_samples/non_diploid_sampling
WORKING_DIR=${PWD}

## Make sure you are in the working directory. Check step 1 for setting ${WORKING_DIR} if it's not set already
cd ${WORKING_DIR}

## Get the script for creating input json files.
wget https://raw.githubusercontent.com/human-pangenomics/hprc_intermediate_assembly/1f61ff0043442d8350a282ef3533def588bee8dc/hpc/launch_from_table.py

WDL_PATH=/private/groups/patenlab/masri/apps/vg_wdl/workflows/haplotype_sampling_customized.wdl
WDL_FILENAME=$(basename ${WDL_PATH})
WDL_NAME=${WDL_FILENAME%%.wdl}

## Make a directory for saving input json files
mkdir -p ${WDL_NAME}_input_jsons
cd ${WDL_NAME}_input_jsons

## Make input json files
## One json will be created per row
python3  ${WORKING_DIR}/launch_from_table.py \
            --data_table ${WORKING_DIR}/data_table.csv \
            --field_mapping ${WORKING_DIR}/input_mapping.csv \
            --workflow_name ${WDL_NAME}
```
#### Run haplotype sampling again (non-diploid)
```
## Make sure you are in the working directory. Check step 1 for setting ${WORKING_DIR} if it's not set already
cd ${WORKING_DIR}

## Get the bash script for running WDLs on Slurm using Toil
wget https://raw.githubusercontent.com/human-pangenomics/hprc_intermediate_assembly/b81bbb9540eaf5632a53faba43be71a0974f14f6/hpc/toil_sbatch_single_machine.sh

## Set environment variables for sbatch
USERNAME="masri"
EMAIL="masri@ucsc.edu"
TIME_LIMIT="70:00:00"

## Partition should be modifed based on the available partitions on the server
PARTITION="long"

## Go to the execution directory
mkdir -p ${WDL_NAME}_logs

## Run jobs arrays
## --array=1-7%7 will make 7 jobs; one per input json file (numbered by row indices in csv file)
sbatch      --job-name=${WDL_NAME}_${USERNAME} \
            --cpus-per-task=32 \
            --mem=128G \
            --mail-user=${EMAIL} \
            --output=${WDL_NAME}_logs/${WDL_NAME}_%A_%a.log \
            --array=1-7%7  \
            --time=${TIME_LIMIT} \
            --partition=${PARTITION} \
            ${WORKING_DIR}/toil_sbatch_single_machine.sh \
            --wdl ${WDL_PATH} \
            --sample_csv  ${WORKING_DIR}/data_table.csv \
            --input_json_path ${WORKING_DIR}/${WDL_NAME}_input_jsons/\${SAMPLE_ID}_${WDL_NAME}.json
```

#### Copy gbz files with _64 and _32 to gs bucket
gs url : `gs://pepper-deepvariant/mobinasri/haplotype_sampling/giab_samples/non_diploid_sampling/`
```
cd /private/groups/patenlab/masri/haplotype_sampling/giab_samples/non_diploid_sampling
for i in $(find . | grep ".gbz$" | grep -e"_32" -e"_64");do gsutil cp $i gs://pepper-deepvariant/mobinasri/haplotype_sampling/giab_samples/non_diploid_sampling/ ;done
```
### Comment 5 : 01/07/2025 
### HG002 illumina and Element (with 16,32,64 haplotypes, non-diploid)
#### Create json files
```
cd /private/groups/patenlab/masri/haplotype_sampling/HG002
WORKING_DIR=${PWD}

## Make sure you are in the working directory. Check step 1 for setting ${WORKING_DIR} if it's not set already
cd ${WORKING_DIR}

## Get the script for creating input json files.
wget https://raw.githubusercontent.com/human-pangenomics/hprc_intermediate_assembly/1f61ff0043442d8350a282ef3533def588bee8dc/hpc/launch_from_table.py

WDL_PATH=/private/groups/patenlab/masri/apps/vg_wdl/workflows/haplotype_sampling_customized.wdl
WDL_FILENAME=$(basename ${WDL_PATH})
WDL_NAME=${WDL_FILENAME%%.wdl}

## Make a directory for saving input json files
mkdir -p ${WDL_NAME}_input_jsons
cd ${WDL_NAME}_input_jsons

## Make input json files
## One json will be created per row
python3  ${WORKING_DIR}/launch_from_table.py \
            --data_table ${WORKING_DIR}/data_table.csv \
            --field_mapping ${WORKING_DIR}/input_mapping.csv \
            --workflow_name ${WDL_NAME}
```

#### Run haplotype sampling 
```
## Make sure you are in the working directory. Check step 1 for setting ${WORKING_DIR} if it's not set already
cd ${WORKING_DIR}

## Get the bash script for running WDLs on Slurm using Toil
wget https://raw.githubusercontent.com/human-pangenomics/hprc_intermediate_assembly/b81bbb9540eaf5632a53faba43be71a0974f14f6/hpc/toil_sbatch_single_machine.sh

## Set environment variables for sbatch
USERNAME="masri"
EMAIL="masri@ucsc.edu"
TIME_LIMIT="70:00:00"

## Partition should be modifed based on the available partitions on the server
PARTITION="long"

## Go to the execution directory
mkdir -p ${WDL_NAME}_logs

## Run jobs arrays
## --array=1-2%2 will make 2 jobs; one per input json file (numbered by row indices in csv file)
sbatch      --job-name=${WDL_NAME}_${USERNAME} \
            --cpus-per-task=32 \
            --mem=128G \
            --mail-user=${EMAIL} \
            --output=${WDL_NAME}_logs/${WDL_NAME}_%A_%a.log \
            --array=1-2%2  \
            --time=${TIME_LIMIT} \
            --partition=${PARTITION} \
            ${WORKING_DIR}/toil_sbatch_single_machine.sh \
            --wdl ${WDL_PATH} \
            --sample_csv  ${WORKING_DIR}/data_table.csv \
            --input_json_path ${WORKING_DIR}/${WDL_NAME}_input_jsons/\${SAMPLE_ID}_${WDL_NAME}.json
```

#### Copy gbz files with _16, _32, and _64 to gs bucket
```
gsutil ls gs://pepper-deepvariant/mobinasri/haplotype_sampling/HG002/non_diploid_sampling/

gs://pepper-deepvariant/mobinasri/haplotype_sampling/HG002/non_diploid_sampling/HG002.element.cloudbreak.1000bp_ins.hap_num_16.CHM13_removed.gbz
gs://pepper-deepvariant/mobinasri/haplotype_sampling/HG002/non_diploid_sampling/HG002.element.cloudbreak.1000bp_ins.hap_num_32.CHM13_removed.gbz
gs://pepper-deepvariant/mobinasri/haplotype_sampling/HG002/non_diploid_sampling/HG002.element.cloudbreak.1000bp_ins.hap_num_64.CHM13_removed.gbz
gs://pepper-deepvariant/mobinasri/haplotype_sampling/HG002/non_diploid_sampling/HG002.novaseq.pcr-free.hap_num_16.CHM13_removed.gbz
gs://pepper-deepvariant/mobinasri/haplotype_sampling/HG002/non_diploid_sampling/HG002.novaseq.pcr-free.hap_num_32.CHM13_removed.gbz
gs://pepper-deepvariant/mobinasri/haplotype_sampling/HG002/non_diploid_sampling/HG002.novaseq.pcr-free.hap_num_64.CHM13_removed.gbz

```

### HG001 illumina and Element (with 16,32,64 haplotypes, non-diploid)
#### Create json files
```
cd /private/groups/patenlab/masri/haplotype_sampling/HG001
WORKING_DIR=${PWD}

## Make sure you are in the working directory. Check step 1 for setting ${WORKING_DIR} if it's not set already
cd ${WORKING_DIR}

## Get the script for creating input json files.
wget https://raw.githubusercontent.com/human-pangenomics/hprc_intermediate_assembly/1f61ff0043442d8350a282ef3533def588bee8dc/hpc/launch_from_table.py

WDL_PATH=/private/groups/patenlab/masri/apps/vg_wdl/workflows/haplotype_sampling_customized.wdl
WDL_FILENAME=$(basename ${WDL_PATH})
WDL_NAME=${WDL_FILENAME%%.wdl}

## Make a directory for saving input json files
mkdir -p ${WDL_NAME}_input_jsons
cd ${WDL_NAME}_input_jsons

## Make input json files
## One json will be created per row
python3  ${WORKING_DIR}/launch_from_table.py \
            --data_table ${WORKING_DIR}/data_table.csv \
            --field_mapping ${WORKING_DIR}/input_mapping.csv \
            --workflow_name ${WDL_NAME}
```

#### Run haplotype sampling 
```
## Make sure you are in the working directory. Check step 1 for setting ${WORKING_DIR} if it's not set already
cd ${WORKING_DIR}

## Get the bash script for running WDLs on Slurm using Toil
wget https://raw.githubusercontent.com/human-pangenomics/hprc_intermediate_assembly/b81bbb9540eaf5632a53faba43be71a0974f14f6/hpc/toil_sbatch_single_machine.sh

## Set environment variables for sbatch
USERNAME="masri"
EMAIL="masri@ucsc.edu"
TIME_LIMIT="70:00:00"

## Partition should be modifed based on the available partitions on the server
PARTITION="long"

## Go to the execution directory
mkdir -p ${WDL_NAME}_logs

## Run jobs arrays
## --array=1-2%2 will make 2 jobs; one per input json file (numbered by row indices in csv file)
sbatch      --job-name=${WDL_NAME}_${USERNAME} \
            --cpus-per-task=32 \
            --mem=128G \
            --mail-user=${EMAIL} \
            --output=${WDL_NAME}_logs/${WDL_NAME}_%A_%a.log \
            --array=1-2%2  \
            --time=${TIME_LIMIT} \
            --partition=${PARTITION} \
            ${WORKING_DIR}/toil_sbatch_single_machine.sh \
            --wdl ${WDL_PATH} \
            --sample_csv  ${WORKING_DIR}/data_table.csv \
            --input_json_path ${WORKING_DIR}/${WDL_NAME}_input_jsons/\${SAMPLE_ID}_${WDL_NAME}.json
```

#### Copy gbz files with _16, _32, and _64 to gs bucket
```
gsutil ls gs://pepper-deepvariant/mobinasri/haplotype_sampling/HG001/non_diploid_sampling/

gs://pepper-deepvariant/mobinasri/haplotype_sampling/HG001/non_diploid_sampling/HG001.element.cloudbreak.1000bp_ins.hap_num_16.CHM13_removed.gbz
gs://pepper-deepvariant/mobinasri/haplotype_sampling/HG001/non_diploid_sampling/HG001.element.cloudbreak.1000bp_ins.hap_num_32.CHM13_removed.gbz
gs://pepper-deepvariant/mobinasri/haplotype_sampling/HG001/non_diploid_sampling/HG001.element.cloudbreak.1000bp_ins.hap_num_64.CHM13_removed.gbz
gs://pepper-deepvariant/mobinasri/haplotype_sampling/HG001/non_diploid_sampling/HG001.novaseq.pcr-free.hap_num_16.CHM13_removed.gbz
gs://pepper-deepvariant/mobinasri/haplotype_sampling/HG001/non_diploid_sampling/HG001.novaseq.pcr-free.hap_num_32.CHM13_removed.gbz
gs://pepper-deepvariant/mobinasri/haplotype_sampling/HG001/non_diploid_sampling/HG001.novaseq.pcr-free.hap_num_64.CHM13_removed.gbz
```
